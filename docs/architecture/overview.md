# Architecture Overview â€” Reference

**Version**: 0.1.0  
**Last Updated**: October 22, 2025

---

## Note

This file is a **placeholder** for Step 1. The complete architecture documentation already exists in:

**ğŸ“„ [/docs/architecture.md](/docs/architecture.md)** (from Step 0 design phase)

That document includes:
- Complete component diagram
- Startup/shutdown sequences
- 3 detailed data flow diagrams (auto-ingest, bg-removal, pack export)
- Job lifecycle state machine
- Directory structure specification

---

## Quick Reference

### System Architecture (Updated STEP 7)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PySide6 UI    â”‚ â† User interaction (Qt widgets)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI        â”‚ â† Async backend (localhost:8971)
â”‚  Backend        â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker    â”‚     â”‚  Offline LLM Agents â”‚ â† STEP 7: llama.cpp
â”‚  Queue     â”‚     â”‚  (CPU-only)         â”‚    (ports 9091-9094)
â”‚  (Jobs)    â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Agent Vision (9091) â”‚
    â”‚              â”‚ Agent Dialog (9092) â”‚
    â”‚              â”‚ Agent Logic  (9093) â”‚
    â”‚              â”‚ Agent Fast   (9094) â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite +  â”‚ â† Data persistence + assets
â”‚  Files     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack (Updated)

- **UI**: PySide6 (Qt 6)
- **Backend**: FastAPI + Uvicorn
- **Database**: SQLite + SQLModel
- **Workers**: ThreadPoolExecutor (default) or RQ (optional)
- **Media**: Pillow, OpenCV, rembg, FFmpeg, pydub
- **LLM Agents**: llama.cpp servers (4 concurrent GGUF models, CPU-only)

### LLM Agent Layer (STEP 7)

**Purpose**: Offline prompt generation and light reasoning

**Architecture**:
- 4 independent llama-server instances (HTTP)
- Each agent bound to specific GGUF model and port
- CPU-only execution (no GPU required)
- Health monitoring via `/api/llm/health`

**Agents**:
1. **Vision** (Port 9091): gemma-3-12b â€” Image analysis, captions
2. **Dialog** (Port 9092): discopop-zephyr-7b â€” Fluency, polish
3. **Logic** (Port 9093): gemma-3n-e4b â€” Reasoning, planning
4. **Fast** (Port 9094): lfm2-1.2b â€” Quick tasks, fallback

**Communication**:
- Backend â†’ Agents: HTTP POST to `http://127.0.0.1:909X/v1/chat/completions`
- Retry logic with exponential backoff
- Fallback routing if primary agent unavailable

**Routing Logic**:
```
Prompt Draft â†’ Logic Agent
Prompt Polish â†’ Dialog Agent  
Image Caption â†’ Vision Agent
Quick Summary â†’ Fast Agent
```

---

**For full architecture, see**: [/docs/architecture.md](/docs/architecture.md)
