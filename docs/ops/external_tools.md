# External Tools Setup

**Version**: 0.1.0  
**Platform**: Windows 10/11  
**Last Updated**: October 22, 2025

---

## Overview

PODStudio relies on external binaries and Python packages for media processing. This doc covers installation and PATH configuration.

---

## Python Dependencies (Required for M3)

**Status:** Implemented in STEP 6

### rembg (Background Removal)

**Purpose:** AI-powered background removal for images

**Installation:**
```powershell
pip install rembg
```

**First Run:**
- U2Net model (176 MB) auto-downloads to rembg cache
- Location: `%USERPROFILE%\.u2net\` on Windows
- Subsequent runs use cached model (no download)

**Usage:** Called automatically by `run_bg_remove_job()`

**Requirements:**
- Python 3.11+
- RAM: 2-4 GB during processing
- CPU: Multi-core recommended (8+ threads)
- GPU: Optional (not yet supported in M3, future enhancement)

**Troubleshooting:**
- **"Model download failed"**: Check internet connection, verify firewall not blocking downloads
- **"Out of memory"**: Close other applications, process smaller images
- **Slow processing**: Normal for CPU mode (5-15s per image)

---

### pydub (Audio Processing)

**Purpose:** Audio loading/decoding for waveform generation

**Installation:**
```powershell
pip install pydub
```

**Dependencies:**
- Requires **FFmpeg** (see below) — pydub uses ffmpeg for decoding

**Usage:** Called by `run_audio_waveform_job()`

**Troubleshooting:**
- **"ffmpeg not found"**: Install FFmpeg and add to PATH
- **"Unsupported format"**: Ensure ffmpeg supports the audio codec

---

### numpy (Array Processing)

**Purpose:** Audio sample downsampling for waveforms

**Installation:**
```powershell
pip install numpy
```

**Usage:** Required by `run_audio_waveform_job()` for audio array processing

---

### Pillow (PIL) (Image Processing)

**Purpose:** Image loading, drawing, saving for background removal and waveforms

**Installation:**
```powershell
pip install Pillow
```

**Usage:** Used by both `run_bg_remove_job()` and `run_audio_waveform_job()`

---

## FFmpeg (Required)

**Purpose**: Video/audio transcoding, metadata extraction

### Installation

1. **Download**: https://www.gyan.dev/ffmpeg/builds/
   - Choose: `ffmpeg-release-essentials.zip`
2. **Extract**: `C:\ffmpeg`
3. **Add to PATH**:
   - Windows Key → "Environment Variables"
   - System Variables → Path → Edit → New
   - Add: `C:\ffmpeg\bin`
   - Click OK (restart PowerShell)

### Verification

```powershell
ffmpeg -version
# Output: ffmpeg version 6.0-...

ffprobe -version
# Output: ffprobe version 6.0-...
```

### Troubleshooting

- **"ffmpeg not recognized"**: Restart PowerShell after editing PATH
- **Missing DLLs**: Download "essentials" build (not "full" or "static")
- **Used by:** Video poster extraction (`run_video_poster_job()`), audio waveform generation (`run_audio_waveform_job()` via pydub)

---

## ExifTool (Recommended, Not Required for M3)

**Purpose**: EXIF metadata extraction from images

**Status:** Optional — Not used in M3 implementation

### Installation

1. **Download**: https://exiftool.org/
   - Windows Executable: `exiftool-12.50.zip`
2. **Extract**: Anywhere (e.g., `C:\Tools\exiftool`)
3. **Rename**: `exiftool(-k).exe` → `exiftool.exe`
4. **Add to PATH**: Same process as FFmpeg

### Verification

```powershell
exiftool -ver
# Output: 12.50 (or current version)
```

### Troubleshooting

- **Slow first run**: ExifTool unpacks on first launch (~5 seconds)
- **Permission errors**: Run PowerShell as Administrator

---

## Real-ESRGAN (Optional, Not Implemented in M3)

**Purpose**: AI-powered 2x/4x image upscaling (GPU-accelerated)

**Status:** Planned for future milestone — Not used in M3

### Installation

1. **Download**: https://github.com/xinntao/Real-ESRGAN/releases
   - Windows: `realesrgan-ncnn-vulkan-<version>-windows.zip`
2. **Extract**: `PODStudio\tools\realesrgan\`
3. **Verify**: `tools\realesrgan\realesrgan-ncnn-vulkan.exe` exists
4. **No PATH needed**: PODStudio calls absolute path from `.env`

### Configuration

Edit `.env`:
```env
MEDIA_TOOLS_REALESRGAN=tools/realesrgan/realesrgan-ncnn-vulkan.exe
```

### Verification

```powershell
.\tools\realesrgan\realesrgan-ncnn-vulkan.exe
# Output: Usage: realesrgan-ncnn-vulkan -i input.jpg -o output.png ...
```

### GPU Requirements

- **Minimum**: NVIDIA GTX 1060 (6GB VRAM) or AMD equivalent
- **Recommended**: RTX 3060+ (8GB+ VRAM)
- **Fallback**: CPU mode (very slow; 10x longer)

### Troubleshooting

- **"Vulkan error"**: Update GPU drivers from NVIDIA/AMD website
- **Slow processing**: Check GPU mode with `-g 0` flag (0 = first GPU)
- **Out of memory**: Reduce batch size or use 2x instead of 4x

---

## audiowaveform (Optional, Not Implemented in M3)

**Purpose**: Generate PNG waveforms for audio files (BBC tool)

**Status:** M3 uses pydub + PIL instead — This tool is optional for future enhancement

**M3 Implementation:** Waveforms generated by `run_audio_waveform_job()` using:
- `pydub` for audio loading
- `numpy` for sample downsampling
- `PIL` (Pillow) for drawing waveform PNG

**Future Enhancement:** Could replace Python implementation with this faster native tool

### Installation

1. **Download**: https://github.com/bbc/audiowaveform/releases
   - Windows: `audiowaveform-<version>-win64.zip`
2. **Extract**: `C:\Tools\audiowaveform`
3. **Add to PATH**: Same process as FFmpeg

### Verification

```powershell
audiowaveform --version
# Output: audiowaveform v1.6.0
```

### Troubleshooting

- **DLL errors**: Install Visual C++ Redistributable from Microsoft

---

## llama.cpp (Required for STEP 7, Offline LLM Agent Layer)

**Purpose**: CPU-only offline LLM inference for prompt generation and light reasoning

**Status:** Implemented in STEP 7 — 4 agent endpoints using GGUF models

### Installation

1. **Download**: https://github.com/ggerganov/llama.cpp/releases
   - Windows: `llama-<version>-bin-win-avx2-x64.zip` (AVX2 build for modern CPUs)
   - Or compile from source with CMake for optimal performance
2. **Extract**: `C:\llama.cpp\` (or any location)
3. **Verify binaries exist**:
   - `llama-server.exe` — HTTP server for model inference
   - `llama-cli.exe` — Command-line inference (testing)
   - `llama-quantize.exe` — Model quantization tool
4. **Add to PATH** (optional):
   - System Variables → Path → Edit → New
   - Add: `C:\llama.cpp\`

### CPU-Only Configuration

PODStudio runs all LLM agents on **CPU only** (no GPU required):

**Thread Tuning:**
- **Ryzen 6800H** (16 threads): Use `--threads 14` (logical cores - 2)
- **Intel i7-12700H** (20 threads): Use `--threads 18`
- **General rule**: `--threads <logical_cores - 2>` to leave headroom for OS

**Memory Requirements:**
- Q4_K_M quantized models: ~2-4 GB RAM per model
- Recommended: 16 GB+ system RAM for 4 concurrent agents
- Context length (ctx-size) impacts RAM: 32K tokens ≈ 2-3 GB

**Performance:**
- Tokens/sec: 15-30 on Ryzen 6800H (Q4 models)
- Latency: 2-5 seconds for typical prompts (100-200 tokens)

### Example Launch Commands

**Agent 1 (Vision/Reader) — Port 9091:**
```powershell
llama-server.exe `
  --model "J:\Models\LLM-Models-2025\models\gemma\gemma-3-12b-q4_k_m.gguf" `
  --mmproj "J:\Models\LLM-Models-2025\models\gemma\mmproj-gemma-3-12b-f16.gguf" `
  --host 127.0.0.1 `
  --port 9091 `
  --ctx-size 32768 `
  --n-gpu-layers 0 `
  --threads 14 `
  --n-batch 512 `
  --timeout 300
```

**Agent 2 (Dialog/Fluency) — Port 9092:**
```powershell
llama-server.exe `
  --model "J:\Models\LLM-Models-2025\models\zephyr\discopop-zephyr-7b-gemma-q4_k_m.gguf" `
  --host 127.0.0.1 `
  --port 9092 `
  --ctx-size 8192 `
  --n-gpu-layers 0 `
  --threads 14 `
  --n-batch 512 `
  --timeout 300
```

**Agent 3 (Logic/Planner) — Port 9093:**
```powershell
llama-server.exe `
  --model "J:\Models\LLM-Models-2025\models\gemma\gemma-3n-e4b-q4_k_m.gguf" `
  --host 127.0.0.1 `
  --port 9093 `
  --ctx-size 16384 `
  --n-gpu-layers 0 `
  --threads 14 `
  --n-batch 512 `
  --timeout 300
```

**Agent 4 (Fast/Fallback) — Port 9094:**
```powershell
llama-server.exe `
  --model "J:\Models\LLM-Models-2025\models\liquid\lfm2-1.2b-q8_0.gguf" `
  --host 127.0.0.1 `
  --port 9094 `
  --ctx-size 4096 `
  --n-gpu-layers 0 `
  --threads 8 `
  --n-batch 256 `
  --timeout 300
```

### Verification

**Check server health:**
```powershell
curl http://127.0.0.1:9091/health
# Output: {"status":"ok","model":"gemma-3-12b"}
```

**Test completion:**
```powershell
curl http://127.0.0.1:9091/v1/completions `
  -H "Content-Type: application/json" `
  -d '{"prompt":"Write a haiku about coding:","max_tokens":50}'
```

### Troubleshooting

- **"llama-server.exe not found"**: Add to PATH or use absolute path
- **Port already in use**: Kill existing process or change `--port`
- **Slow inference**: Reduce `--ctx-size` or increase `--threads`
- **Model not found**: Verify absolute path to .gguf file exists
- **mmproj error (Vision agent)**: Ensure multimodal projector file matches model version
- **Timeout errors**: Increase `--timeout` value for long prompts

### Running All Agents at Startup

See [run_llama_servers.md](run_llama_servers.md) for batch script to launch all 4 agents simultaneously.

---

## Redis (Optional, Not Used in M3)

**Purpose**: Backend for RQ (task queue alternative to ThreadPoolExecutor)

**Status:** M3 uses ThreadPoolExecutor-based job queue — Redis/RQ planned for future milestone

### Installation (Windows Subsystem for Linux)

1. **Install WSL2**:
   ```powershell
   wsl --install
   ```
2. **Install Redis in WSL**:
   ```bash
   sudo apt update
   sudo apt install redis-server
   sudo service redis-server start
   ```
3. **Test**:
   ```bash
   redis-cli ping
   # Output: PONG
   ```

### Configuration

Edit `.env`:
```env
WORKER_BACKEND=rq
REDIS_HOST=localhost
REDIS_PORT=6379
```

### Troubleshooting

- **Connection refused**: Start Redis in WSL: `sudo service redis-server start`
- **Windows-native Redis**: Use Memurai (https://www.memurai.com/) as drop-in replacement

---

## Tool Version Matrix (M3 Status)

| Tool | Tested Version | Required? | Purpose | M3 Status |
|------|----------------|-----------|---------|-----------|
| **Python Packages** | | | | |
| rembg | 2.0.50+ | ✅ Yes (M3) | Background removal | ✅ Implemented |
| pydub | 0.25.1+ | ✅ Yes (M3) | Audio waveforms | ✅ Implemented |
| numpy | 1.24.0+ | ✅ Yes (M3) | Audio processing | ✅ Implemented |
| Pillow (PIL) | 10.0.0+ | ✅ Yes (M3) | Image processing | ✅ Implemented |
| **External Binaries** | | | | |
| llama.cpp | Latest | ✅ Yes (STEP 7) | Offline LLM agents | ✅ Implemented |
| FFmpeg | 6.0+ | ✅ Yes | Video/audio ops | ✅ Required |
| ExifTool | 12.50+ | Recommended | EXIF metadata | 🔜 Future |
| Real-ESRGAN | 0.2.5+ | Optional | GPU upscaling | 🔜 Future |
| audiowaveform | 1.6.0+ | Optional | Fast waveforms | 🔜 Future |
| Redis | 7.0+ | Optional | RQ task queue | 🔜 Future |

---

## Future Tools (Not Yet Supported)

- **yt-dlp**: Download reference videos (v1.1 feature)
- **ImageMagick**: Advanced image manipulation (v1.2 feature)
- **Whisper**: Audio transcription for captions (v2.0 feature)

---

**Next**: See [troubleshooting.md](troubleshooting.md) for common issues.
